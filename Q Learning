{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/s0w0/BRdpG1+Tph4LijL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeamuhid/mashing-learning/blob/main/Q%20Learning\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Nq9ZpNV1yN8J",
        "outputId": "1df929cc-d445-4736-d6b6-e023694d115f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-98f1d6a16ff6>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Training the Q-Learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Initialize state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.95  # Gamma\n",
        "epsilon = 1.0  # Exploration rate\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.01\n",
        "num_episodes = 10000\n",
        "max_steps = 100\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]  # Initialize state\n",
        "    done = False\n",
        "    for step in range(max_steps):\n",
        "        # Exploration-exploitation trade-off\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take the action and observe the reward and new state\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Update Q-table using the Q-Learning formula\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon after each episode\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "# Test the Q-Learning agent\n",
        "env.reset()\n",
        "total_rewards = 0\n",
        "\n",
        "for episode in range(10):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    print(f\"Episode {episode + 1}\")\n",
        "    for step in range(max_steps):\n",
        "        env.render()\n",
        "        action = np.argmax(q_table[state, :])  # Select the best action\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "        total_rewards += reward\n",
        "        state = new_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "print(f\"Total rewards after testing: {total_rewards}\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment (simple grid world)\n",
        "# States: 0 to 5\n",
        "# Actions: 0 (left), 1 (right)\n",
        "# Reward for state 5: +1\n",
        "# All other states: 0\n",
        "\n",
        "states = 6\n",
        "actions = 2  # left, right\n",
        "q_table = np.zeros((states, actions))  # Initialize Q-table\n",
        "\n",
        "# Define rewards\n",
        "rewards = np.array([0, 0, 0, 0, 0, 1])\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.9  # Gamma\n",
        "epsilon = 0.9  # Exploration rate\n",
        "episodes = 1000\n",
        "\n",
        "# Training the agent\n",
        "for episode in range(episodes):\n",
        "    state = 0  # Start from the initial state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose action using epsilon-greedy strategy\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take the action and transition to the next state\n",
        "        new_state = state + 1 if action == 1 else max(state - 1, 0)\n",
        "        reward = rewards[new_state]\n",
        "\n",
        "        # Update Q-value using the Q-Learning formula\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "        if state == states - 1:  # Goal state reached\n",
        "            done = True\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-Table:\")\n",
        "print(q_table)\n",
        "\n",
        "# Testing the agent\n",
        "state = 0\n",
        "print(\"\\nTesting the agent:\")\n",
        "while state != states - 1:\n",
        "    action = np.argmax(q_table[state, :])  # Choose the best action\n",
        "    state = state + 1 if action == 1 else max(state - 1, 0)\n",
        "    print(f\"Moved to state {state}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kNxPZOtyvJN",
        "outputId": "683290c5-45b0-4417-d67b-ea1f5fcffccb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-Table:\n",
            "[[0.59049 0.6561 ]\n",
            " [0.59049 0.729  ]\n",
            " [0.6561  0.81   ]\n",
            " [0.729   0.9    ]\n",
            " [0.81    1.     ]\n",
            " [0.      0.     ]]\n",
            "\n",
            "Testing the agent:\n",
            "Moved to state 1\n",
            "Moved to state 2\n",
            "Moved to state 3\n",
            "Moved to state 4\n",
            "Moved to state 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.95  # Gamma\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 1000\n",
        "max_steps = 100\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]  # Reset the environment\n",
        "    for step in range(max_steps):\n",
        "        # Choose an action using epsilon-greedy strategy\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take the action and observe the reward and new state\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-value\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "# Test the trained Q-Learning agent\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "print(\"Trained Q-Table:\")\n",
        "print(q_table)\n",
        "\n",
        "print(\"\\nTesting the trained agent:\")\n",
        "for step in range(max_steps):\n",
        "    action = np.argmax(q_table[state, :])  # Choose the best action\n",
        "    new_state, reward, done, _, _ = env.step(action)\n",
        "    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
        "    state = new_state\n",
        "    if done:\n",
        "        print(\"Goal reached!\" if reward == 1 else \"Failed!\")\n",
        "        break\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4LK53opCy1_E",
        "outputId": "30c66a73-1173-40e3-8d43-f432038f1253"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84fbc9a09a3f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Training the Q-Learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Choose an action using epsilon-greedy strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.95  # Gamma\n",
        "epsilon = 1.0  # Initial exploration rate\n",
        "max_epsilon = 1.0  # Maximum exploration rate\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "decay_rate = 0.005  # Decay rate for epsilon\n",
        "num_episodes = 5000  # Total number of episodes\n",
        "max_steps = 100  # Max steps per episode\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "rewards = []  # Store rewards for analysis\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]  # Reset environment to start state\n",
        "    total_rewards = 0\n",
        "    for step in range(max_steps):\n",
        "        # Epsilon-greedy policy for exploration-exploitation\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take action and observe outcome\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Update Q-value using the Q-Learning formula\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "        total_rewards += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Reduce epsilon (exploration) after each episode\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "    # Save rewards for analysis\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "# Analyzing performance\n",
        "print(\"Training completed!\")\n",
        "print(f\"Average reward over {num_episodes} episodes: {np.mean(rewards)}\")\n",
        "\n",
        "# Test the Q-Learning agent\n",
        "print(\"\\nTesting the trained agent:\")\n",
        "env.reset()\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "total_test_rewards = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    action = np.argmax(q_table[state, :])  # Choose the best action\n",
        "    new_state, reward, done, _, _ = env.step(action)\n",
        "    print(f\"Step: {step + 1}, State: {state}, Action: {action}, Reward: {reward}\")\n",
        "    state = new_state\n",
        "    total_test_rewards += reward\n",
        "    if done:\n",
        "        print(\"Goal reached!\" if reward == 1 else \"Fell into a hole!\")\n",
        "        break\n",
        "\n",
        "print(f\"Total reward during testing: {total_test_rewards}\")\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "trC0_hfpzIwM",
        "outputId": "404836aa-6514-4c4b-d7f7-1e82bca07dc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9d657afd123e>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Store rewards for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Reset environment to start state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.95  # Gamma\n",
        "epsilon = 1.0  # Initial exploration rate\n",
        "max_epsilon = 1.0  # Maximum exploration rate\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "decay_rate = 0.005  # Decay rate for epsilon\n",
        "num_episodes = 5000  # Total number of episodes\n",
        "max_steps = 100  # Max steps per episode\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "rewards = []  # Store rewards for analysis\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]  # Reset environment to start state\n",
        "    total_rewards = 0\n",
        "    for step in range(max_steps):\n",
        "        # Epsilon-greedy policy for exploration-exploitation\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take action and observe outcome\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Update Q-value using the Q-Learning formula\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "        total_rewards += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Reduce epsilon (exploration) after each episode\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "    # Save rewards for analysis\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "# Analyzing performance\n",
        "print(\"Training completed!\")\n",
        "print(f\"Average reward over {num_episodes} episodes: {np.mean(rewards)}\")\n",
        "\n",
        "# Test the Q-Learning agent\n",
        "print(\"\\nTesting the trained agent:\")\n",
        "env.reset()\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "total_test_rewards = 0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    action = np.argmax(q_table[state, :])  # Choose the best action\n",
        "    new_state, reward, done, _, _ = env.step(action)\n",
        "    print(f\"Step: {step + 1}, State: {state}, Action: {action}, Reward: {reward}\")\n",
        "    state = new_state\n",
        "    total_test_rewards += reward\n",
        "    if done:\n",
        "        print(\"Goal reached!\" if reward == 1 else \"Fell into a hole!\")\n",
        "        break\n",
        "\n",
        "print(f\"Total reward during testing: {total_test_rewards}\")\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "HfxgATzWzSY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "# Initialize the Q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8  # Alpha\n",
        "discount_factor = 0.95  # Gamma\n",
        "epsilon = 1.0  # Exploration rate\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.01\n",
        "num_episodes = 5000\n",
        "max_steps = 100\n",
        "\n",
        "# Metrics\n",
        "rewards = []\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]  # Initialize the state\n",
        "    total_rewards = 0\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Exploration-exploitation trade-off\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take the action and observe the reward and new state\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Update Q-value using the Q-Learning formula\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "        total_rewards += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Reduce epsilon to decrease exploration over time\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "# Calculate and display average reward per 100 episodes\n",
        "rewards_per_hundred = np.split(np.array(rewards), num_episodes / 100)\n",
        "count = 100\n",
        "print(\"\\nAverage reward per 100 episodes:\")\n",
        "for r in rewards_per_hundred:\n",
        "    print(f\"{count}: {np.mean(r)}\")\n",
        "    count += 100\n",
        "\n",
        "# Test the Q-Learning agent\n",
        "print(\"\\nTesting the trained agent:\\n\")\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "total_rewards = 0\n",
        "\n",
        "time.sleep(1)  # Pause for better visualization\n",
        "for step in range(max_steps):\n",
        "    env.render()  # Visualize the environment\n",
        "    action = np.argmax(q_table[state, :])  # Choose the best action\n",
        "    new_state, reward, done, _, _ = env.step(action)\n",
        "    state = new_state\n",
        "    total_rewards += reward\n",
        "    time.sleep(0.5)  # Slow down the visualization\n",
        "\n",
        "    if done:\n",
        "        env.render()\n",
        "        if reward == 1:\n",
        "            print(\"\\nGoal reached!\")\n",
        "        else:\n",
        "            print(\"\\nFell into a hole or ran out of time.\")\n",
        "        break\n",
        "\n",
        "print(f\"Total reward during testing: {total_rewards}\")\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HsBCl3B5ze4r",
        "outputId": "dcda2d1a-fecd-4c6b-cede-bf4be85c1fbc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9f9d230d0a8e>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Training the Q-Learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Initialize the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}